{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "%load_ext autoreload\n",
    "%matplotlib inline\n",
    "%autoreload 2\n",
    "\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['MKL_NUM_THREADS'] = '4'\n",
    "os.environ['GOTO_NUM_THREADS'] = '4'\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "import theano\n",
    "theano.config.openmp = True\n",
    "\n",
    "os.environ['THEANO_FLAGS'] = 'device=cpu,blas.ldflags=-lblas -lgfortran'\n",
    "# os.environ[\"THEANO_FLAGS\"] = \"mode=FAST_RUN,optimizer=fast_compile,device=gpu0,floatX=float32\"\n",
    "\n",
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "import six.moves.cPickle as pickle\n",
    "import os, re, json\n",
    "\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Layer,Merge,Reshape,Dense,Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import activations, initializations\n",
    "from six.moves import range\n",
    "from six.moves import zip\n",
    "\n",
    "from utils_pack.utils import pickle_in,pickle_out,ensure_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 300 \n",
    "skip_top = 0 \n",
    "nb_epoch = 10\n",
    "dim_proj = 256 \n",
    "\n",
    "save = True\n",
    "load_model = False\n",
    "load_tokenizer = False\n",
    "train_model = True\n",
    "model_name = \"model_1\"\n",
    "\n",
    "\n",
    "save_dir = os.path.join(\"../models\",\"food_terms_embedding_models\",model_name)\n",
    "model_load_fname = \"food2vec_model.pkl\"\n",
    "model_save_fname = \"foo2vec_model.pkl\"\n",
    "tokenizer_fname = \"food_tokenizer.pkl\"\n",
    "ensure_dir(save_dir)\n",
    "\n",
    "# dataset_recipe_plus_ingredient_flat\n",
    "data_path = os.path.join(\"../\",\"datasets\",\"flat_dataset_for_word_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def text_generator(path=data_path):\n",
    "\n",
    "#     recipes = pickle_in(path)\n",
    "#     recipes_flat =[]\n",
    "#     for i, recipe in enumerate(recipes):\n",
    "#         recipe_flat = \" \".join([item for sublist in recipe for item in sublist])\n",
    "#         recipes_flat.append(recipe_flat.encode('utf-8'))\n",
    "#         print recipe_flat\n",
    "\n",
    "#     for j,recipe in enumerate(recipes_flat):\n",
    "#         if j % 10000 == 0:\n",
    "#             print(j)\n",
    "#         yield recipe\n",
    "\n",
    "def text_generator(path=data_path):\n",
    "\n",
    "    recipes_flat = pickle_in(path)\n",
    "\n",
    "    for j,recipe in enumerate(recipes_flat):\n",
    "\n",
    "        recipe = [item.encode(\"utf-8\") for item in recipe]\n",
    "        recipe = \" \".join(recipe)\n",
    "        if j % 100 == 0:\n",
    "            print(j)\n",
    "        yield recipe\n",
    "        \n",
    "def my_filter():\n",
    "    f = \"\\n\"\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit tokenizer...\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "Save tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# model management\n",
    "if load_tokenizer:\n",
    "    print('Load tokenizer...')\n",
    "    tokenizer = six.moves.cPickle.load(open(os.path.join(save_dir, tokenizer_fname), 'rb'))\n",
    "else:\n",
    "    print(\"Fit tokenizer...\")\n",
    "    tokenizer = text.Tokenizer(nb_words=max_features,filters=my_filter(),split=\" \")\n",
    "    tokenizer.fit_on_texts(text_generator())\n",
    "    if save:\n",
    "        print(\"Save tokenizer...\")\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        pickle_out(os.path.join(save_dir, tokenizer_fname),tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "word = Sequential()\n",
    "word.add(Embedding(max_features,dim_proj,input_length=1,init='uniform'))\n",
    "\n",
    "context = Sequential()\n",
    "context.add(Embedding(max_features,dim_proj,input_length=1, init='uniform'))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Merge([word, context], mode='dot',dot_axes=2))\n",
    "model.add(Reshape((1,), input_shape=(1,1)))\n",
    "# model.add(Dense(1024))\n",
    "model.compile(loss='mse', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "('Epoch', 0)\n",
      "----------------------------------------\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "899/964 [==========================>...] - ETA: 72s - loss: 0.0755 Saving model...\n",
      "('Samples seen:', 36356606)\n",
      "----------------------------------------\n",
      "('Epoch', 1)\n",
      "----------------------------------------\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-1793aaf97a79>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m                 \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"int32\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m                 \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jakubczakon/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[0;32m    498\u001b[0m         return self.model.train_on_batch(x, y,\n\u001b[0;32m    499\u001b[0m                                          \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 500\u001b[1;33m                                          class_weight=class_weight)\n\u001b[0m\u001b[0;32m    501\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m     def test_on_batch(self, x, y,\n",
      "\u001b[1;32m/home/jakubczakon/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1166\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1167\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1168\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1169\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1170\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jakubczakon/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    516\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 518\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jakubczakon/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training process\n",
    "if train_model:\n",
    "    if load_model:\n",
    "        print('Load model...')\n",
    "        model = pickle.load(open(os.path.join(save_dir, model_load_fname), 'rb'))\n",
    "          \n",
    "    sampling_table = sequence.make_sampling_table(max_features)\n",
    "\n",
    "    for e in range(nb_epoch):\n",
    "        print('-'*40)\n",
    "        print('Epoch', e)\n",
    "        print('-'*40)\n",
    "\n",
    "        progbar = generic_utils.Progbar(tokenizer.document_count)\n",
    "        samples_seen = 0\n",
    "        losses = []\n",
    "        \n",
    "        for i, seq in enumerate(tokenizer.texts_to_sequences(text_generator())):\n",
    "            # get skipgram couples for one text in the dataset\n",
    "            couples, labels = sequence.skipgrams(seq, max_features, window_size=50, negative_samples=10.)\n",
    "            if couples:\n",
    "                # one gradient update per sentence (one sentence = a few 1000s of word couples)\n",
    "                X = np.array(couples, dtype=\"int32\")\n",
    "                X1 = X[:,0]\n",
    "                X2 = X[:,1]\n",
    "                Y = np.array(labels, dtype=\"int32\")\n",
    "                \n",
    "                loss = model.train_on_batch([X1,X2], Y)\n",
    "                losses.append(loss)\n",
    "                if len(losses) % 100 == 0:\n",
    "                    progbar.update(i, values=[(\"loss\", np.mean(losses))])\n",
    "                    losses = []\n",
    "                samples_seen += len(labels)\n",
    "        if save:\n",
    "            print(\"Saving model...\")\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            pickle_out(os.path.join(save_dir, model_save_fname),word)\n",
    "        \n",
    "        print('Samples seen:', samples_seen)\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "    if save:\n",
    "        print(\"Saving model...\")\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        pickle_out(os.path.join(save_dir, model_save_fname),word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# recover the embedding weights trained with skipgram:\n",
    "weights = word.layers[0].get_weights()[0]\n",
    "\n",
    "weights[:skip_top] = np.zeros((skip_top, dim_proj))\n",
    "norm_weights = np_utils.normalize(weights)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "reverse_word_index = dict([(v, k) for k, v in list(word_index.items())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_embedding_dict = {}\n",
    "for key, value in word_index.iteritems():\n",
    "    if value<max_features:\n",
    "        i+=1\n",
    "        word_embedding_dict[key] = norm_weights[value,:]\n",
    "pickle_out(os.path.join(save_dir,\"word_embedding_dict.pkl\"),word_embedding_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embed_word(w):\n",
    "    i = word_index.get(w)\n",
    "    if (not i) or (i<skip_top) or (i>=max_features):\n",
    "        return None\n",
    "    return norm_weights[i]\n",
    "\n",
    "def closest_to_point(point, nb_closest=10):\n",
    "    proximities = np.dot(norm_weights, point)\n",
    "    tups = list(zip(list(range(len(proximities))), proximities))\n",
    "    tups.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [(reverse_word_index.get(t[0]), t[1]) for t in tups[:nb_closest]]  \n",
    "\n",
    "def closest_to_word(w, nb_closest=10):\n",
    "    i = word_index.get(w)\n",
    "    if (not i) or (i<skip_top) or (i>=max_features):\n",
    "        return []\n",
    "    return closest_to_point(norm_weights[i].T, nb_closest)\n",
    "def index_to_word(i):\n",
    "    i = word_index.get(w)\n",
    "    if (not i) or (i<skip_top) or (i>=max_features):\n",
    "        return []\n",
    "    return closest_to_point(norm_weights[i].T, nb_closest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('====', 'kg')\n",
      "('kg', 1.0)\n",
      "('adicion', 0.78753746)\n",
      "('junt', 0.78607404)\n",
      "('sirv', 0.78587294)\n",
      "('minutos.', 0.78486925)\n",
      "('retir', 0.77535379)\n",
      "('cebol', 0.77527612)\n",
      "('cozinh', 0.77165669)\n",
      "('grand', 0.77012157)\n",
      "('sal', 0.76517504)\n",
      "('====', '1')\n",
      "('1', 0.99999994)\n",
      "('leit', 0.81393635)\n",
      "('2', 0.8138063)\n",
      "('mistur', 0.81235182)\n",
      "('4', 0.81155324)\n",
      "('3', 0.81103861)\n",
      "('1/2', 0.80718273)\n",
      "('5', 0.80522406)\n",
      "('x\\xc3\\xadc', 0.79982591)\n",
      "('colh', 0.79965991)\n",
      "('====', 'vermelh')\n",
      "('vermelh', 0.99999988)\n",
      "('piment\\xc3\\xa3', 0.77589607)\n",
      "('verd', 0.76038849)\n",
      "('pic', 0.75509858)\n",
      "('grand', 0.75445622)\n",
      "('20', 0.75436878)\n",
      "('salsinh', 0.7518909)\n",
      "('rodel', 0.75160682)\n",
      "('m\\xc3\\xa9di', 0.75119042)\n",
      "('quant', 0.75085509)\n"
     ]
    }
   ],
   "source": [
    "words = [\n",
    "\"kg\",\n",
    "\"1\",\n",
    "\"vermelh\",\n",
    "]\n",
    "\n",
    "for w in words:\n",
    "    res = closest_to_word(w)\n",
    "    print('====', w)\n",
    "    for r in res:\n",
    "        print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
