{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import array\n",
    "import collections\n",
    "import io\n",
    "import itertools\n",
    "import cPickle as pickle\n",
    "\n",
    "import scipy.sparse as sp\n",
    "import numbers\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from glove import Glove\n",
    "import nltk\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import linear_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glove_model_filepath = os.path.join(\"../models\",\"glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GloveExtended(Glove):\n",
    "    \n",
    "    @classmethod\n",
    "    def load_stanford(cls, filename):\n",
    "        \"\"\"\n",
    "        Load model from the output files generated by\n",
    "        the C code from http://nlp.stanford.edu/projects/glove/.\n",
    "        The entries of the word dictionary will be of type\n",
    "        unicode in Python 2 and str in Python 3.\n",
    "        \"\"\"\n",
    "\n",
    "        dct = {}\n",
    "        vectors = array.array('d')\n",
    "\n",
    "        # Read in the data.\n",
    "        with io.open(filename, 'r', encoding='utf-8') as savefile:\n",
    "            for i, line in enumerate(savefile):\n",
    "                tokens = line.split(' ')\n",
    "\n",
    "                word = tokens[0]\n",
    "                entries = tokens[1:]\n",
    "\n",
    "                dct[word] = i\n",
    "                vectors.extend(float(x) for x in entries)\n",
    "\n",
    "        # Infer word vectors dimensions.\n",
    "        no_components = len(entries)\n",
    "        no_vectors = len(dct)\n",
    "\n",
    "        # Set up the model instance.\n",
    "        instance = GloveExtended() # here i change the instance to my current class name\n",
    "        instance.no_components = no_components\n",
    "        instance.word_vectors = (np.array(vectors)\n",
    "                                 .reshape(no_vectors,\n",
    "                                          no_components))\n",
    "        instance.word_biases = np.zeros(no_vectors)\n",
    "        instance.add_dictionary(dct)\n",
    "\n",
    "        return instance\n",
    "    \n",
    "    def most_similar_to_word_vector(self, word_vector, number=5):\n",
    "        if self.word_vectors is None:\n",
    "            raise Exception('Model must be fit before querying')\n",
    "\n",
    "        if self.dictionary is None:\n",
    "            raise Exception('No word dictionary supplied')\n",
    "\n",
    "        return self._similarity_query(word_vector, number)[1:]\n",
    "    \n",
    "    def similarity(self,word1,word2):\n",
    "        vector1 = self.word_vectors[glv.dictionary[word1]]\n",
    "        vector2 = self.word_vectors[glv.dictionary[word2]]\n",
    "        vector1 =  vector1.reshape(1,vector1.shape[0])\n",
    "        vector2 =  vector2.reshape(1,vector2.shape[0])\n",
    "        return cosine_similarity(vector1,vector2)[0][0]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.8 s, sys: 1.16 s, total: 41 s\n",
      "Wall time: 41.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "glv = GloveExtended.load_stanford(glove_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def glove_mean_vector(word_list):\n",
    "    vector_list = []\n",
    "    for word in word_list:\n",
    "        vector_list.append(glv.word_vectors[glv.dictionary[word]])\n",
    "    return np.mean(np.stack(vector_list),axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'mexican', 0.8550674735062006),\n",
       " (u'venezuela', 0.84968989901002756),\n",
       " (u'colombia', 0.849031767897101),\n",
       " (u'peru', 0.84464826419836103),\n",
       " (u'chile', 0.84392901369166462),\n",
       " (u'puerto', 0.83626278721957459),\n",
       " (u'rico', 0.81946955303500257),\n",
       " (u'cuba', 0.81252054023253284),\n",
       " (u'guatemala', 0.81138109161210081),\n",
       " (u'panama', 0.80967559123222543),\n",
       " (u'brazil', 0.80768006021078875),\n",
       " (u'costa', 0.80469082550556659),\n",
       " (u'bolivia', 0.79278940477367765),\n",
       " (u'ecuador', 0.79041663921967487),\n",
       " (u'argentina', 0.7792276485929357),\n",
       " (u'rica', 0.77619745108727212),\n",
       " (u'honduras', 0.77205183034845315),\n",
       " (u'nicaragua', 0.76766200804852081),\n",
       " (u'salvador', 0.7545959658101794),\n",
       " (u'spain', 0.75137645100707895),\n",
       " (u'philippines', 0.75002143680699229),\n",
       " (u'dominican', 0.73882111230848269),\n",
       " (u'san', 0.73853777921996433),\n",
       " (u'paraguay', 0.73149171475091945),\n",
       " (u'california', 0.73001444734832066),\n",
       " (u'uruguay', 0.72562287128822289),\n",
       " (u'chilean', 0.71203996543230963),\n",
       " (u'francisco', 0.71055504193806407),\n",
       " (u'america', 0.70622742421037732),\n",
       " (u'venezuelan', 0.70284586489237033),\n",
       " (u'neighboring', 0.70099396166276351),\n",
       " (u'states', 0.69860799883180857),\n",
       " (u'republic', 0.6951329848263651),\n",
       " (u'caribbean', 0.69262896602970392),\n",
       " (u'region', 0.69014455507438521),\n",
       " (u'southeastern', 0.68912592922834059),\n",
       " (u'veracruz', 0.68760572495337569),\n",
       " (u'buenos', 0.68643846631846506),\n",
       " (u'southern', 0.68572715091700143),\n",
       " (u'cuban', 0.6856892809527827),\n",
       " (u'southeast', 0.68222963470836062),\n",
       " (u'aires', 0.68182520734169783),\n",
       " (u'florida', 0.67884079608220771),\n",
       " (u'mexicans', 0.67726446345872315),\n",
       " (u'colombian', 0.67703993082660407),\n",
       " (u'province', 0.67671226683961305),\n",
       " (u'diego', 0.6741713242127072),\n",
       " (u'coast', 0.673708142105095),\n",
       " (u'eastern', 0.6726682157779762)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_vector = glove_mean_vector([\"mexico\"])\n",
    "glv.most_similar_to_word_vector(mean_vector,number=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_all_connections_query(model,query_word,number=5,similarity_cut = 0.25):\n",
    "    query = model.most_similar(query_word,number=number)\n",
    "    all_words = [item[0] for item in query]+[query_word]\n",
    "    \n",
    "    i=0\n",
    "    all_words_dict = {}\n",
    "    for word in all_words:\n",
    "        all_words_dict[word]=i\n",
    "        i+=1\n",
    "    nodes_list = []\n",
    "    for key in all_words_dict:\n",
    "        nodes_list.append({\"name\":key})    \n",
    "\n",
    "    all_links_list = []\n",
    "    for word1,word2 in itertools.combinations(all_words,r=2):\n",
    "        distance =1- model.similarity(word1,word2)\n",
    "\n",
    "        if distance<similarity_cut:\n",
    "            all_links_list.append({\"source\":all_words_dict[word1],\n",
    "                               \"target\":all_words_dict[word2],\n",
    "                                   \"value\":distance\n",
    "                                  })\n",
    "        \n",
    "    final_dict = {\"nodes\":nodes_list,\n",
    "                  \"links\":all_links_list\n",
    "                 }\n",
    "    return final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_distance_pairs =  get_all_connections_query(glv,\"people\",number=30,similarity_cut=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_file = os.path.join(\"../d3_visualization\",\"word_distance.json\")\n",
    "with open(output_file, 'w') as fp:\n",
    "    json.dump(word_distance_pairs, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
