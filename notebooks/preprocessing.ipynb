{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "import cPickle as pickle\n",
    "\n",
    "import os\n",
    "import json\n",
    "from copy import copy\n",
    "\n",
    "from itertools import groupby,chain,tee,izip,islice\n",
    "from collections import Iterable,Counter\n",
    "from operator import itemgetter \n",
    "\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import RSLPStemmer,SnowballStemmer\n",
    "from nltk import PunktSentenceTokenizer,FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import UnigramTagger,BigramTagger\n",
    "\n",
    "from utils_pack.utils import pickle_in,pickle_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and clean data from crawlers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filepath = os.path.join(\"../crawlers\",\"cozinhabrasileira\",\"cozinhabrasileira\",\"spiders\",\"items3.json\")\n",
    "data = []\n",
    "with open(filepath) as json_file:\n",
    "    for line in json_file:       \n",
    "        if line[0]==\"[\":\n",
    "            line = line[1:]\n",
    "            line = line[:-2]\n",
    "        elif line[-1]==\"]\":\n",
    "            line = line[:-1]\n",
    "        else:\n",
    "            line = line[:-2]\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "titles = []\n",
    "ingredients = []\n",
    "recipes = []\n",
    "for entry in data:\n",
    "    if entry['title'] ==[] or entry['ingredients'] ==[] or entry['recipe'] == []:\n",
    "        continue\n",
    "    \n",
    "    titles.append(entry['title'])\n",
    "    ingredients.append(entry['ingredients'])\n",
    "    recipes.append(entry['recipe']) \n",
    "    \n",
    "ingredients_cleaned = [[i.split() for i in ing] for ing in ingredients]\n",
    "\n",
    "titles_tokens= [i[0].split() for i in titles]\n",
    "\n",
    "recipes_cleaned = [[r.split() for r in rec if r.split()!=[]] for rec in recipes]\n",
    "\n",
    "final_dataset = zip(titles,titles_tokens,ingredients_cleaned,recipes_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full dataset is just a python zipped list of titles,tokenized titles,tokenized ingredients and tokenized recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_filepath = os.path.join(\"../datasets\",\"full_dataset_cozinhabrasileira.pkl\")\n",
    "pickle_out(output_filepath,final_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_filepath = os.path.join(\"../datasets\",\"full_dataset_cozinhabrasileira.pkl\")\n",
    "dataset = pickle_in(input_filepath)\n",
    "titles,titles_tokens,ingredients_cleaned,recipes_cleaned=zip(*dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing\n",
    "\n",
    "With low number of data points stemming and dropping stop words is important to increse token frequencies in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "stemmer_Snow = SnowballStemmer(\"portuguese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titles_stemmed = [[stemmer_Snow.stem(word) for word in title if word not in stopwords] \n",
    "                  for title in titles_tokens]\n",
    "recipes_stemmed = [[[stemmer_Snow.stem(word) for word in sentence if word not in stopwords] \n",
    "                    for sentence in recipe] for recipe in recipes_cleaned]\n",
    "ingredients_stemmed = [[[stemmer_Snow.stem(word) for word in sentence if word not in stopwords] \n",
    "                    for sentence in recipe] for recipe in ingredients_cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_dataset_list = zip(titles,titles_stemmed,recipes_stemmed,ingredients_stemmed)\n",
    "pickle_out(os.path.join(\"../datasets\",\"full_dataset_list.pkl\"),full_dataset_list)\n",
    "full_dataset_dict={}\n",
    "for t,ts,r,i in full_dataset_list:\n",
    "    full_dataset_dict[t[0]] = {'title':ts,'recipe':r,'ingredient':i}\n",
    "pickle_out(os.path.join(\"../datasets\",\"full_dataset_dict.pkl\"),full_dataset_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a flat corpus for word embedding training based on recipies ingredients and titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flat_dataset = []\n",
    "for t,ts,r,i in full_dataset_list:\n",
    "    r = [item for sublist in r for item in sublist]\n",
    "    i = [item for sublist in i for item in sublist]\n",
    "    full_flat = r+i+ts\n",
    "    flat_dataset.append(full_flat)\n",
    "pickle_out(os.path.join(\"../datasets\",\"flat_dataset_for_word_embeddings.pkl\"),\n",
    "           flat_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to detect a token that could be a quantity unit like pinch of salt or glass of water I will subtitute every digit in the ingredients dataset with the word \"number\" and then train the model to find the words that coexisted with the word \"number\" in the ingredient dataset. Similarly I will subsitute every digit in the recipe dataset with the word \"time\" to try and find time units. Usually the quantities are not specified within the recipe since they were listed in the ingredients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contains_number(inputString):\n",
    "    return any(char.isdigit() for char in inputString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "size_quant_detect_ingredient_dataset = [[[\"number\" if contains_number(word) else word for word in sentence ] \n",
    "                               for sentence in recipe ] for recipe in ingredients_stemmed]\n",
    "\n",
    "flat_quant_detect_ingredient_dataset= []\n",
    "for recipe in size_quant_detect_ingredient_dataset:\n",
    "    for ingredients in recipe:\n",
    "        if \"number\" not in ingredients:\n",
    "            ingredients = [\"number\"]+ingredients\n",
    "        flat_quant_detect_ingredient_dataset.append(ingredients)\n",
    "        \n",
    "pickle_out(os.path.join(\"../datasets\",\"size_quantity_detection_dataset.pkl\"),flat_quant_detect_ingredient_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_quant_detect_recipe_dataset = [[[\"time\" if contains_number(word) else word for word in sentence ] \n",
    "                               for sentence in recipe ] for recipe in recipes_stemmed]\n",
    "\n",
    "flat_quant_detect_recipe_dataset= []\n",
    "for recipe in time_quant_detect_recipe_dataset:\n",
    "    for ingredients in recipe:\n",
    "        flat_quant_detect_recipe_dataset.append(ingredients)\n",
    "pickle_out(os.path.join(\"../datasets\",\"time_quantity_detection_dataset.pkl\"),flat_quant_detect_recipe_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
