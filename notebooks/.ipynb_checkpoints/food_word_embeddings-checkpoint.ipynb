{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "%load_ext autoreload\n",
    "%matplotlib inline\n",
    "%autoreload 2\n",
    "\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['MKL_NUM_THREADS'] = '4'\n",
    "os.environ['GOTO_NUM_THREADS'] = '4'\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n",
    "import theano\n",
    "theano.config.openmp = True\n",
    "\n",
    "os.environ['THEANO_FLAGS'] = 'device=cpu,blas.ldflags=-lblas -lgfortran'\n",
    "# os.environ[\"THEANO_FLAGS\"] = \"mode=FAST_RUN,optimizer=fast_compile,device=gpu0,floatX=float32\"\n",
    "\n",
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "import six.moves.cPickle as pickle\n",
    "import os, re, json\n",
    "\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Layer,Merge,Reshape,Dense,Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import activations, initializations\n",
    "from six.moves import range\n",
    "from six.moves import zip\n",
    "\n",
    "from utils_pack.utils import pickle_in,pickle_out,ensure_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 300 \n",
    "skip_top = 0 \n",
    "nb_epoch = 10\n",
    "dim_proj = 256 \n",
    "\n",
    "save = True\n",
    "load_model = False\n",
    "load_tokenizer = False\n",
    "train_model = True\n",
    "model_name = \"model_1\"\n",
    "\n",
    "\n",
    "save_dir = os.path.join(\"../models\",\"food_terms_embedding_models\",model_name)\n",
    "model_load_fname = \"food2vec_model.pkl\"\n",
    "model_save_fname = \"foo2vec_model.pkl\"\n",
    "tokenizer_fname = \"food_tokenizer.pkl\"\n",
    "ensure_dir(save_dir)\n",
    "\n",
    "# dataset_recipe_plus_ingredient_flat\n",
    "data_path = os.path.join(\"../\",\"datasets\",\"flat_dataset_for_word_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def text_generator(path=data_path):\n",
    "\n",
    "#     recipes = pickle_in(path)\n",
    "#     recipes_flat =[]\n",
    "#     for i, recipe in enumerate(recipes):\n",
    "#         recipe_flat = \" \".join([item for sublist in recipe for item in sublist])\n",
    "#         recipes_flat.append(recipe_flat.encode('utf-8'))\n",
    "#         print recipe_flat\n",
    "\n",
    "#     for j,recipe in enumerate(recipes_flat):\n",
    "#         if j % 10000 == 0:\n",
    "#             print(j)\n",
    "#         yield recipe\n",
    "\n",
    "def text_generator(path=data_path):\n",
    "\n",
    "    recipes_flat = pickle_in(path)\n",
    "\n",
    "    for j,recipe in enumerate(recipes_flat):\n",
    "\n",
    "        recipe = [item.encode(\"utf-8\") for item in recipe]\n",
    "        recipe = \" \".join(recipe)\n",
    "        if j % 100 == 0:\n",
    "            print(j)\n",
    "        yield recipe\n",
    "        \n",
    "def my_filter():\n",
    "    f = \"\\n\"\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit tokenizer...\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "Save tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# model management\n",
    "if load_tokenizer:\n",
    "    print('Load tokenizer...')\n",
    "    tokenizer = six.moves.cPickle.load(open(os.path.join(save_dir, tokenizer_fname), 'rb'))\n",
    "else:\n",
    "    print(\"Fit tokenizer...\")\n",
    "    tokenizer = text.Tokenizer(nb_words=max_features,filters=my_filter(),split=\" \")\n",
    "    tokenizer.fit_on_texts(text_generator())\n",
    "    if save:\n",
    "        print(\"Save tokenizer...\")\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        pickle_out(os.path.join(save_dir, tokenizer_fname),tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "word = Sequential()\n",
    "word.add(Embedding(max_features,dim_proj,input_length=1,init='uniform'))\n",
    "\n",
    "context = Sequential()\n",
    "context.add(Embedding(max_features,dim_proj,input_length=1, init='uniform'))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Merge([word, context], mode='dot',dot_axes=2))\n",
    "model.add(Reshape((1,), input_shape=(1,1)))\n",
    "# model.add(Dense(1024))\n",
    "model.compile(loss='mse', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "('Epoch', 0)\n",
      "----------------------------------------\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "399/964 [===========>..................] - ETA: 610s - loss: 0.0770"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-77719280e65e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;31m# get skipgram couples for one text in the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mcouples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mskipgrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcouples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[1;31m# one gradient update per sentence (one sentence = a few 1000s of word couples)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jakubczakon/anaconda2/lib/python2.7/site-packages/keras/preprocessing/sequence.pyc\u001b[0m in \u001b[0;36mskipgrams\u001b[1;34m(sequence, vocabulary_size, window_size, negative_samples, shuffle, categorical, sampling_table)\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[0mcouples\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabulary_size\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb_negative_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcategorical\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnb_negative_samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training process\n",
    "if train_model:\n",
    "    if load_model:\n",
    "        print('Load model...')\n",
    "        model = pickle.load(open(os.path.join(save_dir, model_load_fname), 'rb'))\n",
    "          \n",
    "    sampling_table = sequence.make_sampling_table(max_features)\n",
    "\n",
    "    for e in range(nb_epoch):\n",
    "        print('-'*40)\n",
    "        print('Epoch', e)\n",
    "        print('-'*40)\n",
    "\n",
    "        progbar = generic_utils.Progbar(tokenizer.document_count)\n",
    "        samples_seen = 0\n",
    "        losses = []\n",
    "        \n",
    "        for i, seq in enumerate(tokenizer.texts_to_sequences(text_generator())):\n",
    "            # get skipgram couples for one text in the dataset\n",
    "            couples, labels = sequence.skipgrams(seq, max_features, window_size=50, negative_samples=10.)\n",
    "            if couples:\n",
    "                # one gradient update per sentence (one sentence = a few 1000s of word couples)\n",
    "                X = np.array(couples, dtype=\"int32\")\n",
    "                X1 = X[:,0]\n",
    "                X2 = X[:,1]\n",
    "                Y = np.array(labels, dtype=\"int32\")\n",
    "                \n",
    "                loss = model.train_on_batch([X1,X2], Y)\n",
    "                losses.append(loss)\n",
    "                if len(losses) % 100 == 0:\n",
    "                    progbar.update(i, values=[(\"loss\", np.mean(losses))])\n",
    "                    losses = []\n",
    "                samples_seen += len(labels)\n",
    "        if save:\n",
    "            print(\"Saving model...\")\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            pickle_out(os.path.join(save_dir, model_save_fname),word)\n",
    "        \n",
    "        print('Samples seen:', samples_seen)\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "    if save:\n",
    "        print(\"Saving model...\")\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        pickle_out(os.path.join(save_dir, model_save_fname),word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's test time!\n"
     ]
    }
   ],
   "source": [
    "# recover the embedding weights trained with skipgram:\n",
    "weights = word.layers[0].get_weights()[0]\n",
    "\n",
    "weights[:skip_top] = np.zeros((skip_top, dim_proj))\n",
    "norm_weights = np_utils.normalize(weights)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "reverse_word_index = dict([(v, k) for k, v in list(word_index.items())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embed_word(w):\n",
    "    i = word_index.get(w)\n",
    "    if (not i) or (i<skip_top) or (i>=max_features):\n",
    "        return None\n",
    "    return norm_weights[i]\n",
    "\n",
    "def closest_to_point(point, nb_closest=10):\n",
    "    proximities = np.dot(norm_weights, point)\n",
    "    tups = list(zip(list(range(len(proximities))), proximities))\n",
    "    tups.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [(reverse_word_index.get(t[0]), t[1]) for t in tups[:nb_closest]]  \n",
    "\n",
    "def closest_to_word(w, nb_closest=10):\n",
    "    i = word_index.get(w)\n",
    "    if (not i) or (i<skip_top) or (i>=max_features):\n",
    "        return []\n",
    "    return closest_to_point(norm_weights[i].T, nb_closest)\n",
    "def index_to_word(i):\n",
    "    i = word_index.get(w)\n",
    "    if (not i) or (i<skip_top) or (i>=max_features):\n",
    "        return []\n",
    "    return closest_to_point(norm_weights[i].T, nb_closest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('====', 'kg')\n",
      "('kg', 1.0)\n",
      "('sal', 0.79152799)\n",
      "('alho', 0.78716141)\n",
      "('bem', 0.78396344)\n",
      "('quente.', 0.78109741)\n",
      "('2', 0.7783398)\n",
      "('cozinh', 0.77300835)\n",
      "('sirv', 0.77262282)\n",
      "('fic', 0.77066565)\n",
      "('reserv', 0.76827341)\n",
      "('====', '1')\n",
      "('1', 0.99999994)\n",
      "('de', 0.93757033)\n",
      "('2', 0.8990761)\n",
      "('sal', 0.87212569)\n",
      "('3', 0.86868668)\n",
      "('1/2', 0.86316025)\n",
      "('em', 0.84614706)\n",
      "('colher', 0.83985615)\n",
      "('mistur', 0.83951569)\n",
      "('4', 0.8302359)\n",
      "('====', 'vermelh')\n"
     ]
    }
   ],
   "source": [
    "''' the resuls in comments below were for: \n",
    "    5.8M HN comments\n",
    "    dim_proj = 256\n",
    "    nb_epoch = 2\n",
    "    optimizer = rmsprop\n",
    "    loss = mse\n",
    "    max_features = 50000\n",
    "    skip_top = 100\n",
    "    negative_samples = 1.\n",
    "    window_size = 4\n",
    "    and frequency subsampling of factor 10e-5. \n",
    "'''\n",
    "\n",
    "words = [\n",
    "\"kg\",\n",
    "\"1\",\n",
    "\"vermelh\",\n",
    "]\n",
    "\n",
    "for w in words:\n",
    "    res = closest_to_word(w)\n",
    "    print('====', w)\n",
    "    for r in res:\n",
    "        print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
