https://github.com/fchollet/keras/issues/1029

https://github.com/fchollet/keras/issues/1110

########################################################################################################

Thanks for the nice figure and more clear explanation. That is what I want to mean. I have checked the examples of the Keras but all of them are used for classification and no sequence labeling examples are given. I don't know why.

@joetigger Here is a demo for word sequence labeling. It show what the data format you need for the last many-to-many scenario in EdwardRaff's figure
maxlen = 2

batch_size = 1
nb_word = 4
nb_tag = 2

X_train = [[1,2],[1,3]]#two sequences, one is [1,2] and another is [1,3]
Y_train = [[[0,1],[1,0]],[[0,1],[1,0]]] #the output should be 3D and one-hot for softmax output with categorical_crossentropy
X_test = [[1,2],[1,3]]
Y_test = [[[0,1],[1,0]],[[0,1],[1,0]]]

X_train = sequence.pad_sequences(x_train, maxlen=maxlen)
X_test = sequence.pad_sequences(x_test, maxlen=maxlen)

Y_train = np.asarray(y_train, dtype='int32')
Y_test = np.asarray(y_test, dtype='int32')

model = Sequential()
model.add(Embedding(nb_word, 128))
model.add(LSTM(128, return_sequences=True))
model.add(TimeDistributedDense(nb_tag))
model.add(Activation('softmax'))

rms = RMSprop()
model.compile(loss='categorical_crossentropy', optimizer=rms)

model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=4, show_accuracy=True)
res = model.predict_classes(X_test)
print('res',res)

in your example , why not use this format?
X={(1,2,3),(2,3,4),(3,4,5),(4,5,6),...}; y={4,5, 6,7, ...}

####################################################################################################

from keras.models import Sequential  
from keras.layers.core import TimeDistributedDense, Activation, Dropout  
from keras.layers.recurrent import GRU
import numpy as np

def _load_data(data, steps = 40):  
    docX, docY = [], []
    for i in range(0, data.shape[0]/steps-1):
        docX.append(data[i*steps:(i+1)*steps,:])
        docY.append(data[(i*steps+1):((i+1)*steps+1),:])
    return np.array(docX), np.array(docY)

def train_test_split(data, test_size=0.15):  
    #    This just splits data to training and testing parts
    X,Y = _load_data(data)
    ntrn = round(X.shape[0] * (1 - test_size))
    perms = np.random.permutation(X.shape[0])
    X_train, Y_train = X.take(perms[0:ntrn],axis=0), Y.take(perms[0:ntrn],axis=0)
    X_test, Y_test = X.take(perms[ntrn:],axis=0),Y.take(perms[ntrn:],axis=0)
    return (X_train, Y_train), (X_test, Y_test) 

np.random.seed(0)  # For reproducability
data = np.genfromtxt('closingAdjLog.csv', delimiter=',')  # data = a TxN matrix
(X_train, y_train), (X_test, y_test) = train_test_split(np.flipud(data))  # retrieve data
print "Data loaded."

in_out_neurons = 20  
hidden_neurons = 200

model = Sequential()  
model.add(GRU(hidden_neurons, input_dim=in_out_neurons, return_sequences=True))
model.add(Dropout(0.2))
model.add(TimeDistributedDense(in_out_neurons))  
model.add(Activation("linear"))  
model.compile(loss="mean_squared_error", optimizer="rmsprop") 
print "Model compiled."

# and now train the model. 
model.fit(X_train, y_train, batch_size=30, nb_epoch=200, validation_split=0.1)  
predicted = model.predict(X_test)  
print np.sqrt(((predicted - y_test) ** 2).mean(axis=0)).mean()  # Printing RMSE 